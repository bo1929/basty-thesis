\setlength{\parindent}{0pt}
\chapter{\bf RESULTS}\label{chapter:results}
\section{Employing the Pipeline and  Evaluation}\label{section:employing-proposed-pipeline}
A total of 11 experiments (7 wild-type sleep and 4 sleep-deprived) and their annotations are split into 10 training experiments and 1 test experiment by leaving one out for each split.
We report results for each split and evaluate varying performance for different splits.

Our pipeline starts with the feature extraction stage, where raw tracking data generated by DeepLabCut is used to generate meaningful behavioral representations.
Body parts used to extract features are proboscis, haltere, thorax, head (left and right), and three midlegs (left and right).

In the preprocessing step, we use oriented pose values for body parts with left and right counterparts, as described in Section~\ref{section:oriented-pose-values}.
Then, in order to detect occluded body parts, we set the low confidence score threshold to $0.075$.
In addition to the hard threshold of confidence score, the moving median window size $\tau$ is determined to be $15$ frames ($0.5$ seconds), and threshold $\tau$ is set to $15$ microns.
If the position of a body part exceeds the median of the window centered around it by threshold tau $\tau$, or if the confidence score is lower than the threshold, then it is deemed that there exists occluded body parts at the corresponding time point, as described in Section~\ref{section:detecting-occlusions}.
Finally, linear interpolation is used for the imputation of such frames with occluded body parts.
After that to conclude preprocessing step, firstly, a median filter with a window size of $6$ frames ($0.2$ seconds), and then a boxcar filter with a window size of size $6$ frames ($0.2$ seconds) is applied to reduce the tracking noise and smooth the signal, but without smoothing behaviors exhibited by rapid and short-duration movements.
Aligning different orientations and transforming the frames to be egocentric did not result in performance improvement in our case, hence, we did not transform frames as egocentric.

In the next step, we computed snapshot features and gradient features as described in Section~\ref{section:spatiotemporal-fetaures}. Details of the snapshot and gradient features are given in Table~\ref{table:spatiotemporal-features}.
In order to generate postural dynamics from snapshot features, we applied wavelet transformation at $20$ different frequency channels dyadically spaced between $1$ Hz and $20$ Hz (see Equation~\ref{equation:dyadically-spaced-spectrum} for determining spectrum frequencies). Different timescales are normalized as given in Equation~\ref{equation:wavelet-normalization}. % \citep{liu_rectification_2007}
After flattening the tensor representation of the postural dynamics (with shape $T \times 13 \times 20)$, and $\textnormal{L}_1$ normalization of frames, we ended up with a $13 \times 20 = 260$ dimensional behavioral representation matrix $\V{\hat{W}}$.
Similarly, for gradient features, moving mean values are computed for a single timescale, that is $33$ milliseconds, resulting in $9 \times 1 = 9$ dimensional behavioral representation matrix $\V{\hat{M}}^\mu$.
It is only used in the activity detection stage for constructing $\Dormancy$ set.

\begin{table}[htb!]
	\begin{adjustbox}{width=1\textwidth}
		\begin{tabular}{c l l}
			\toprule
			                                         & \multicolumn{1}{c}{\textbf{Snapshot Features}} & \multicolumn{1}{c}{\textbf{Gradient Features}} \\
			\cmidrule(lr){2-2}\cmidrule(lr){3-3}
			\multirow{7}{*}{Distance between}        & haltere and origin                             & head and proboscis                             \\
			                                         & proboscis and origin                           & thorax and proboscis                           \\
			                                         & thorax and origin                              & thorax and origin                              \\
			                                         & head and proboscis                             &                                                \\
			                                         & haltere and thorax                             &                                                \\
			                                         & (average) midlegs and origin                   &                                                \\
			                                         & (average) midlegs and thorax                   &                                                \\
			\cmidrule(lr){2-2}\cmidrule(lr){3-3}
			\multirow{3}{*}{Cartesian components of} & haltere                                        & head                                           \\
			                                         & head                                           & proboscis                                      \\
			                                         & thorax                                         & thorax                                         \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Computed spatio-temporal features. \label{table:spatiotemporal-features}}
\end{table}

After computing behavioral representations and constructing $\V{\hat{W}}$ for each experiment, the activity detection stage of our pipeline takes place.
Here, reported recall scores are for the frame set $\MicroActivity$, and it is desired to achieve high recall scores, except for the \QuiescentOther category.
Recall score for \QuiescentOther category corresponds to the false-positive rate.
We evaluate both unsupervised and supervised approaches, recall, and accuracy values are shown in Figure~\ref{figure:detection-performance}.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.495\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/ActivityDetectionPerformance-Supervised.pdf}
		\caption{Supervised detection.}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[b]{0.495\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/ActivityDetectionPerformance-Unsupervised.pdf}
		\caption{Unsupervised detection.}
	\end{subfigure}%
	\caption[Performance summary of micro-activity detection.]{Performance summary of micro-activity detection.
		The red line indicates the macro average of accuracy scores achieved for each split.
		High recall scores are desired for behavioral categories, as opposed to the false-positive rate for the negative category of \QuiescentOther.
		Supervised and unsupervised detections are given respectively in the left subfigure and the right subfigure. \label{figure:detection-performance}}
\end{figure}

For unsupervised activity detection, the threshold $c$ is set to the decision boundary $\lambda_1$ of two Gaussian components for the construction of the $\Dormancy$ set, as described in Section~\ref{section:detecting-activities}.
Similarly, thresholds $c_i$ for each $\V{u}_i$ value are determined as the first decision boundary $\lambda_1$ of $3$ Gaussian components sorted by their means.
The resulting detection performance is quite poor for the switch-like behavior of haltere since \HaltereSwitch behavior occurs very subtly and is similar to quiescent frames.
8 out of 11 splits have a \HaltereSwitch recall score of less than $0.5$.
Moreover, for two of the sleep-deprived experiment splits (FlyF19SDs), \ProboscisPumping recall scores are below $0.15$, which makes it impossible to proceed with a successful mapping.

In the supervised approach, a random forest of decision trees \citep{breiman_random_2001} is utilized with $10$ estimators. The maximum depth of each tree is $5$ and the criterion is Gini impurity.
The recall scores of \ProboscisPumping category are always greater than $0.97$, which implies that we only lose an insignificant amount of annotated frames.
Similarly, the recall score of \Feeding drops below $0.95$ only once to $0.80$.
For \ProboscisPumping, 6 out of 11 splits have a recall score greater than $0.9$, and performance is relatively poor for only two of the splits ($0.61$ and $0.56$ recall).
\CHaltereSwitch is again the most challenging behavioral category. However, recall is often greater than $0.75$ as opposed to the unsupervised approach.
Considering its superior performance over the unsupervised approach, we proceed to behavior mapping by employing supervised detection.

After activity detection, the next stage is behavior mapping, where the frames in the set $\MicroActivity$ are mapped to behavioral categories.
The first step is the computation of behavioral embeddings.
In this step, the semi-supervised pair embeddings described in the Section~\ref{section:pair-embeddings} computed for each annotated and unannotated experiment pair.
We set the embedding space dimension to 2, which $260$ dimensional behavioral representation matrix $\V{\hat{W}}$ is reduced to.
The number of neighbors parameter of UMAP is set to $75$, the minimum distance parameter is determined as $0$, and the distance metric is Hellinger distance (Equation~\ref{equation:hellinger-distance}).

Then, utilizing the generated behavioral embeddings, a nearest neighbors analysis is performed in order to assign behavioral scores to each unannotated frame.
In each semi-supervised embedding, we computed $25$ nearest annotated neighbors of the frames, and contributions of the neighbors are weighted disproportional to their distances, using the Euclidean distance, as formally given in the Equation~\ref{equation:nn-weights}.
After that, each nearest neighbor weights are normalized with the $\log_2$ number of occurrences of the corresponding behavioral category, and then $\textnormal{L}_1$ normalized to make behavioral weights sum up to $1$ (see Equation ~\ref{equation:behavioral-weight-occurence-normalization},~\ref{equation:behavioral-weight-activation}).
Finally, we sum behavioral weights obtained from different ``views'' (i.e., annotated experiment), and $\textnormal{L}_1$ normalization is applied again to end up with the final behavioral score values.

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\linewidth]{figures/BehavioralScoresDistributions_perBehavior.pdf}
	\caption[Distributions of behavioral score values of each behavioral category for all splits together.]{Distributions of behavioral score values of each behavioral category for all splits.
		Each box-plot column demonstrates the behavioral score distributions of target behaviors for the corresponding annotation.\label{figure:behavioral-score-distributions}}
\end{figure}

The resulting score vectors are used for classification and also utilized as confidence scores.
In Figure~\ref{figure:behavioral-score-distributions}, behavioral score distribution for each category is given for all splits together.
Except for the \Feeding category, the median score of the correct target behavioral category is greater than $0.5$, and the computed behavioral scores successfully characterize categories.
Compared to other behaviors, \Feeding has the poorest performance, with a median correct score of $0.2$.
The \PosturalAdjustment category wrongly has the highest median score for \Feeding.
Behavioral scores are also helpful for revealing similarities and dissimilarities between behavioral categories and can be helpful for determining appropriate spatio-temporal feature sets.
For instance, \Grooming and \PosturalAdjustment categories are target behaviors having the second-highest behavioral scores for each other, as expected.
When the \Grooming behavior is short, it becomes more similar to short-duration moving and \PosturalAdjustment behavior.
Frames with \Feeding behavior annotation are often misidentified as \PosturalAdjustment, \Grooming, and \ProboscisPumping.
As the movement of the proboscis is common both for \Feeding and \ProboscisPumping, confusion with the \ProboscisPumping is expected for \Feeding.
Another important observation reveals the robustness of the nearest neighbor analysis to the imbalanced distribution of behavioral categories.
The normalization of behavioral weights with the number of occurrences of behaviors favors behavior categories that are rare and relatively short-duration (such as \HaltereSwitch). However, we observe that \HaltereSwitch scores successfully capture the correct behavioral category, and do not exceed $0.1$ for wrong categories.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.99\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/PRC_ROC-DActfiltered.pdf}
		\caption{ROC and precision-recall curves for frames detected as micro-activity. \label{figure:ROC-PRC-Act}}
	\end{subfigure}%

	\begin{subfigure}[b]{0.99\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/PRC_ROC-DAnnfiltered.pdf}
		\caption{ROC and precision-recall curves for annotated frames. \label{figure:ROC-PRC-Ann}}
	\end{subfigure}%
	\caption[Performance summary of behavior mapping demonstrated using receiver operating characteristic curve and precision-recall curve.
	]{Performance summary of behavior mapping demonstrated using receiver operating characteristic curve and precision-recall curve.
		The weighted averages of ROC and precision-recall curves are computed by interpolation.
		Curves for both frames estimated as micro-activity and annotated frames are given respectively in Figures~\ref{figure:ROC-PRC-Act}, ~\ref{figure:ROC-PRC-Ann}. \label{figure:ROC-PRC}}
\end{figure}

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.5\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/AUC_ROC-DActfiltered.pdf}
		\caption{AUC scores for $\MicroActivity$ set. \label{figure:AUC-ROC-Act}}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[b]{0.5\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/AUC_ROC-DAnnfiltered.pdf}
		\caption{AUC scores for annotated frames. \label{figure:AUC-ROC-Ann}}
	\end{subfigure}%
	\caption[Performance summary of behavior mapping with the area under curve scores of ROC.]
	{Performance summary of behavior mapping with the area under curve scores of ROC.
		The red line indicates the macro average of AUC scores for each split.
		Scores for both frames estimated as micro-activity and annotated frames are given respectively in Figures~\ref{figure:AUC-ROC-Act}, ~\ref{figure:AUC-ROC-Ann}.
		\label{figure:AUC}}
\end{figure}

We compute precision-recall and receiver operating characteristic curves for each split by employing the computed behavioral scores, and also calculate the interpolated weighted averages of the curves, see Figure~\ref{figure:ROC-PRC} and Figure~\ref{figure:AUC}.
Evaluations are done by considering two subsets of frames: the subset of frames annotated as one of the behavioral categories (excluding the frames annotated as \QuiescentOther) and the $\MicroActivity$ set.
The latter one contains false positive micro-activities, which are quantified as the recall score of the \QuiescentOther category in Figure~\ref{figure:detection-performance}.
False positive micro-activities affect \HaltereSwitch detection performance most. Weighted mean precision score of \HaltereSwitch decreases by ${\sim}0.5$ for fixed interpolated recall score of $0.65$.
This dramatic performance difference also indicates the importance of achieving low false-positive rate for \QuiescentOther category in the activity detection stage.
Since the application of our pipeline to unannotated data requires tackling with false-positive activities as well, the following comments are made on the evaluation that is done by considering $\Microactivity$ set.

The most robust detection is achieved for \ProboscisPumping and \PosturalAdjustment behavioral categories, respectively, achieving $\textnormal{F-}1$ greater than $0.8$ and $0.85$ scores for some splits.
The maximum $\textnormal{F-}1$ score achieved for \Grooming is $0.60$ for FlyF1-03082020164520 split, whereas $\textnormal{F-}1$ scores of \HaltereSwitch behavioral category do not exceed $0.4$.
AUC scores for the \ProboscisPumping often exceed $0.95$, implying robust detection for almost all the splits.
We observe that detection performance varies greatly among different experiments. This variation can be due to several reasons, and usually reasons are related to tracking performance and annotation quality, as well as behavioral repertoires of flies.
Considering all splits together, we achieve a macro average of $0.8$ AUC score with a one-versus-rest approach.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.545\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/BehavioralScores-RepertoireDifference.pdf}
		\caption{Behavioral score values, before $\textnormal{L}_1$ normalization. \label{figure:repertoire-score-comparison}}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[b]{0.445\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/Entropy-RepertoireDifference.pdf}
		\caption{Entropy values. The red line is the mean. \label{figure:repertoire-entropy-comparison}}
	\end{subfigure}%
	\caption[Histogram of entropy values and box-plot of the behavioral scores computed using one unannotated and two annotated experiments with varying behavioral repertoires.]{Histogram of entropy values and box-plot of behavioral scores computed using one unannotated and two annotated experiments with varying behavioral repertoires.
		Here, the behavioral repertoire of FlyF1-03082020 is predicted separately with two different annotated experiments, namely FlyF14-08172021 and FlyM13-08172021.
		Behavioral scores and entropy values are computed for the \HaltereSwitch behavior.
		The latter one (FlyM13) lacks the \HaltereSwitch behavior, and as a result, behavioral scores tend to have higher entropy.
		Results demonstrate the ability of the proposed pipeline to detect and discover unseen unannotated behavioral categories using behavioral scores.\label{figure:repertoire-difference}}
\end{figure}

As discussed in Chapter~\ref{chapter:introduction}, the behavioral repertoire of the fruit fly is much richer than the behavioral categories that we considered.
Moreover, there exists a great amount of variation among flies' behavioral repertoire and behavioral visits (see Section~\ref{section:analyzing-behavioral-repertoires}).
Thus, we also investigate our pipeline's ability to detect unseen behavioral categories by utilizing behavioral scores.
Such an ability is important for avoiding misleading the user of the pipeline. It may also be desired to reconsider defined and annotated behavioral categories, and may further lead to interesting biological observations.
To this end, we consider the following experimental setting: behavior mapping of an unannotated experiment (FlyF1-03082020) is done separately for two annotated experiments, namely FlyF14-0817202 and FlyM13-08172021.
Then, behavioral scores of the FlyF1-03082020's frames with \HaltereSwitch annotation are compared between FlyF14-0817202 guided mapping and FlyM13-08172021 guided mapping.
As it can be seen from Figure~\ref{figure:time-spent-in-behaviors}, FlyF1-03082020 and FlyF14-0817202 spent ${\sim}4$ minutes by exhibiting switch-like haltere movement behavior, whereas FlyM13-08172021 spent no more than $5$ seconds.
Therefore, compared to FlyF14-0817202, the \HaltereSwitch is an unseen and lacking behavioral category for FlyM13-08172021.
We expect to identify this difference using behavioral scores.
Indeed, behavioral scores reflect the difference in the behavioral repertoires of the annotated flies.
As it can be seen in Figure~\ref{figure:repertoire-score-comparison}, behavioral scores obtained based on FlyF14-0817202's behavioral repertoire are much more confident about the correct behavioral category, whereas behavioral scores obtained based on FlyM13-08172021's behavioral repertoire are more uniform-like and total score is often shared among multiple behavioral categories.
Using the entropy of the behavioral score, one can quantify the existence of unseen and new behavioral categories in a more systematic way.
In Figure~\ref{figure:repertoire-entropy-comparison}, we compare entropy distributions of the behavioral scores obtained using annotated experiments FlyM13-08172021 and FlyF14-0817202.
We can immediately see that scores obtained based on the FlyM13-08172021's repertoire have higher entropy, with a mean of $0.41$, whereas mean entropy for the other one is $0.15$.
For instance, if the entropy of a frame's score vector is greater than $0.2$, it may be recommended to perform a manual check and investigate the reason.
Accordingly, set of annotated behavioral categories might be extended.

\section{Analyzing Behavioral Repertoires}\label{section:analyzing-behavioral-repertoires}
In this section, we analyze the collected data of sleep experiments to discover how behaviors that we are interested in are exhibited and organized.
Starting from a broad perspective, the briefest description of sleep experiments can be done by quantifying the displacement of the flies placed in the chamber.
In Figure~\ref{figure:displacement}, we plot velocity-based feature vectors, $\V{u}$, between ZT10-ZT02 and ZT10-ZT06, respectively for wild-type sleep and sleep-deprived experiments.
We observe that the long dormancy epochs are interrupted by relatively short macro activity bouts during sleep.
In addition to short macro activity bouts, we observe very small displacements all over the night, which indicates there exist micro-activities other than major positional and postural changes.
Hence, a more fine-grained analysis and detailed examination are necessary to gain insight, as this work attempts to achieve.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.995\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/Velocity-WT-1T.pdf}
		\caption{Wild type.}
	\end{subfigure}%

	\begin{subfigure}[b]{0.995\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/Velocity-SD-1T.pdf}
		\caption{Sleep-deprived.}
	\end{subfigure}%
	\caption[Overall displacement values over entire experiments.]{Overall displacement values over entire experiments.
		Displacement of the body reveals long dormancy and sleep epochs, and macro-activities.
		Displacement values are computed as described in Equation~\ref{equation:displacement} and smoothed with a rolling mean of a 1-minute window.\label{figure:displacement}}
\end{figure}

\begin{figure}[htb!]
	\centering\includegraphics[width=0.995\linewidth]{figures/ActivityBinned-Ann-WT-5T.pdf}
	\caption[Binned temporal heatmap of activities.]{Binned temporal heatmap of activities.
		Each bin is $5$ minutes, corresponding to $9000$ frames.
		Activity value is computed as the ratio of the number of annotated frames and the total number of frames in that bin. \label{figure:heatmap-microactivity}}
\end{figure}

Using annotated behavioral categories, we further examine more closely how activities manifested during sleep.
In Figure~\ref{figure:heatmap-microactivity}, experiments are temporally divided into 5-minute bins, and activity is quantified as the ratio between the number of annotated frames to the total number of frames ($900$ frames). Such a quantification provides us a view of sleep which demonstrates that many activities occur without the displacement of the fly's body.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.995\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/FeatureDistributions_perBehavior-Ann.pdf}
		\caption{Summation of all frequency channels of behavioral representation value for each spatio-temporal feature.\label{figure:feature-distributions}}
	\end{subfigure}%

	\centering
	\begin{subfigure}[b]{0.995\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/BoutDurationDistributions-Ann.pdf}
		\caption{Kernel density estimations of bout durations for each behavioral category. The variance of bout durations within and across behavioral categories demonstrates the rich behavioral repertoire. \label{figure:bout-durations}}
	\end{subfigure}%

	\centering
	\begin{subfigure}[b!]{0.995\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/TimeSpent-perBehavior-Ann.pdf}
		\caption{Time spent while exhibiting each behavioral category for all experiments. \label{figure:time-spent-in-behaviors}}
	\end{subfigure}%
	\caption{Summary of behavioral repertoires, demonstrated using both spatial and temporal characteristics. \label{figure:summary-behavior-characteristics}}
\end{figure}

In addition to the temporal organization of the activities, we are also interested in the characteristics of each behavioral category such as total time spent, bout durations, kinematics, and feature value distributions.
In order to visualize feature distributions of each behavioral category, we sum behavioral representation values of all frequency channels for each spatio-temporal feature.
For a single frame, resulting values sum up to $1$ as they are $\textnormal{L}_1$ normalized.
In Figure~\ref{figure:feature-distributions}, we see how behavioral categories are defined by spatial characteristics.
As expected, proboscis-related features play a significant role in characterizing \Feeding and \ProboscisPumping.
Similarly, features related to leg and thorax are more apparent for \Grooming and \PosturalAdjustment.

Behaviors differ in terms of bout durations as well.
Kernel density estimations of bout duration distributions for each behavioral category are given in Figure~\ref{figure:bout-durations}.
\CHaltereSwitch rarely exceeds several seconds, and \PosturalAdjustment behaviors usually occur within ${\sim}4$ seconds, and both have relatively low variance $4.4$, and $8.2$ seconds, respectively.
\CFeeding, \ProboscisPumping, and \Grooming tend to have longer bouts with greater variances.
For example, the bout duration distribution of \Grooming has a variance of $15$ seconds, and the mean bout duration is $6.2$ seconds.

\begin{figure}[ht!]
	\centering
	\begin{subfigure}[b]{0.995\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/Heatmap_BehavioralVisit-Ann.pdf}
		\caption{Rate of behavioral visits during the sleep within each $5$ sleep percentage bin. Each row is normalized, to sum up to $1$.}
	\end{subfigure}%

	\centering
	\begin{subfigure}[b]{0.995\linewidth}
		\centering\includegraphics[width=\linewidth]{figures/CumulativeLine_BehavioralVisit-Ann.pdf}
		\centering\includegraphics[width=\linewidth]{figures/MeanCumRofC_BehavioralVisit-Ann.pdf}
		\caption{Cumulative behavioral visits for each behavior, behaviors with less than 2 bouts are excluded.}
	\end{subfigure}%

	\caption[Demonstration of behavioral visits during the sleep.]{Demonstration of behavioral visits during the sleep.
		Ethograms of flies are aligned by dividing the longest dormant epoch into 100 bins, corresponding to the sleep percentages.
		The total number of behavioral visits is normalized by the total number of bouts for each fly and behavioral category, separately. \label{figure:temporal-organization-behavior}}

	% \centering
	% \begin{subfigure}[b]{0.45\linewidth}
	% 	\centering
	% 	\includegraphics[width=\linewidth]{figures/FractionTime-Microactivity.pdf}
	% 	\caption{Fraction of time spent with micro-activities in dormant epochs, comparing wild-type experiments and sleep-deprived experiments.}
	% \end{subfigure}%
\end{figure}

Another important aspect of fruit flies' behavioral repertoires is the total time spent while exhibiting each behavior (see Figure~\ref{figure:time-spent-in-behaviors}).
\CGrooming is the most exhibited behavior in terms of total time spent, with an average of $19.9$ minutes, and a standard deviation of $5.2$ minutes.
We observe a great variance for time spent exhibiting \HaltereSwitch among experiments, the average, and the standard deviation are $4.5$ minutes and $5.9$ respectively.

Characterization of underlying changes in behaviors is essential for understanding sleep, and thus, we also examine behavioral visits that occurred during flies' sleep.
To evaluate different experiments jointly, we align sleep bouts by dividing the longest dormancy epoch into bins.
In Figure~\ref{figure:temporal-organization-behavior}, behavioral visits are plotted with a joint $x$-axis, namely the sleep percentages.
The most immediate observation about the temporal organization of the behavioral categories is of \HaltereSwitch.
The occurrence of \HaltereSwitch is not uniform along the time, for 3 sleep-deprived and 3 wild-type sleep experiments $90\%$ of the \HaltereSwitch occurrences are after $40$ sleep percentage.
For 8 out of 11 experiments, more than $40\%$ of the switch-like haltere movements occur between $60$ and $80$ sleep percentages.
Moreover, for 4 out of 11 experiments, $80\%$ of \HaltereSwitch behaviors are observed between this interval.
Another observation is that \Grooming behaviors are more likely to occur in the last $40$ sleep percentage and very unlikely to occur within the first $20$ sleep percentage.
