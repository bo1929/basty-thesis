\chapter{Feature Extraction}
For a single experiment data, i.e., single fruit fly recorded between ZT10 and ZT2 (zeitgeber time 10 and 2), feature extraction consists of four consecutive steps, where the input in one stage is the output of the previous one.
The input of the first step is the raw output signal of the tracking and pose estimation model, in our case, the output of DeepLabCut, a toolbox for markerless pose estimation.
The feature extraction steps are as follows:

\begin{enumerate}
	\item Constructing pose values and preprocessing; dealing with occluded body parts, alignment of different orientations, filtering and imputation.
	\item Computing spatio-temporal features, such as distances between body parts, velocity, angular velocity from body part positions.
	\item Computing dynamic postural features by extending spatio-temporal features to multiple time-scales using wavelet transformation and sliding window statistics.
	\item Computing normalized high-dimensional behavioral representations.
\end{enumerate}

Matrices $\V{X} \in \mathbb{R}^{T \times N}$ and $\V{Y} \in \mathbb{R}^{T \times N}$ denotes a multivariate time series for $x$ and $y$ cartesian components of two-dimensional video recordings that are collected for $N$ tracked body parts of the fly on $T$ consecutive time stamps by a pose estimation model.
This multivariate time series matrices $\V{X}$ and $\V{Y}$ are the raw input data that goes into the first step of the feature extraction.
Note that the number of body parts, $N$, must be the same among all experiments conducted with different fruit flies, but the number of time stamps, $T$, might differ.
Each column of the $\V{X}=[\V{x}_1, \cdots, \V{x}_N]^\top$ and $\V{Y}=[\V{y}_1, \cdots, \V{y}_N]^\top$ can be written respectively as follows;

\begin{align*}
	\V{y}_i & = (y_{i,1}, y_{i,2}, \dots, y_{i,t-1}, y_{i,t}, y_{i,t+1}, \dots, y_{i,T}), \\
	\V{x}_i & = (x_{i,1}, x_{i,2}, \dots, x_{i,t-1}, x_{i,t}, x_{i,t+1}, \dots, x_{i,T}).
\end{align*}

Here $i$ denotes the index of the body part, e.g., leg tip or proboscis.

In addition to $\V{X}$ and $\V{Y}$, a pose estimation model may report prediction scores for each tracked body part at each time step, which is the case for DeepLabCut as well.
$L \in \mathbb{R}^{N \times T}$ denotes the time series of prediction scores, each column of the $L=[\V{l_1}, \cdots, \V{l_N}]^\top$ can be written as follows;

\begin{equation*}
	\V{l}_i = (l_{i,1}, l_{i,2}, \dots, l_{i,t-1}, l_{i,t}, l_{i,t+1}, \dots, l_{i,T}).
\end{equation*}

The prediction scores tend to be very low when the body part is not visible.
Thus, $L$ provides valuable information about the occluded body parts.
In the Section~\ref{section:dealing-with-occluded-body-parts}, how $L$ is incorporated into construction of pose values is described in detail.

\section{Preprocessing}
The goal of in this step is to preprocess the signal, which involves filtering and imputation of certain video frames.
But in addition to this, there are a couple of optional procedures that can be beneficial for our task of learning stereotypical behaviors.
These additional procedures deal with occluded body parts of the fly, alignment of the fly orientations and defining new points of interest.

\subsection{Dealing with Occluded Body Parts}\label{section:dealing-with-occluded-body-parts}
As mentioned in the Section~\ref{section:challanges}, the two-dimensionality of the video recordings introduces a number of important challenges, and one of them is the problem of occluded body parts.
There are many types of occlusions, and some of them can be informally described as follows.
One examples is short occlusions resulting from postural changes.
Imputation of the time series $\V{X}$ and $\V{Y}$ for such short occlusions is relatively easy since the number of consecutive missing data points are not many.
However, this is not the case for long occlusions, which usually occur when the fly is dormant for a long period of time.
Especially for the body parts which have left and right counterparts, it is usual that the orientation of the fly results in one of the counterparts being occluded in long dormant epochs.
We follow multiple approaches to handle different type occlusions; namely imputation and elimination of corresponding data-points.
Before describing those approaches, we define a criterion for being occluded.

\subsubsection{Oriented Pose Values for Body Parts with Left \& Right Counterparts}
If the fly is oriented perpendicular to the camera perspective, as in Figure~\ref{figure:perpendicular-orientation}, then one of the left and right body parts is often occluded.
In other orientations (e.g., Figure~\ref{figure:parallel-orientation} and Figure~\ref{figure:oblique-orientation}),  both of them or none of them might be occluded as well.
However, in the conducted experiments, flies usually choose to stay dormant perpendicular to the camera perspective in long dormant epochs, as mentioned in Chapter~\ref{chapter:expt-data-collection}.
In such cases, one can concede to use only one of the left and right counterparts to construct pose values.
Therefore, this optional step is included in the behavioral mapping pipeline to reduce pose values of body parts with left and right counterparts to a single value.

We use prediction scores to determine which body part should be used to compute oriented pose values.
Let $i$ and $j$ be a pair of body parts which are left and right counterparts of each other, e.g., left haltere and right haltere.
Then, one can use the $\V{l}_i$ and $\V{l}_j$ to predict if one of them is occluded at a particular time step $t$.
Let $\orient{\V{x}_i, \V{x}_j}$ ($\orient{\V{y}_i, \V{y}_j}$) be a new pose vector which will be computed based on $\V{x}_i$ and $\V{x}_j$ ($\V{y}_i$ and $\V{y}_j$), e.g., a vector of oriented haltere pose values, composed of left haltere and right haltere pose values.
The following conditional procedures are proposed to compute oriented pose values from left and right pose values by deciding the orientation of the fly for a counterpart body pair. The procedures below can be used separately, or successively.

\begin{itemize}
	\item If $l_{i,t} - l_{j,t} \geq  \epsilon$, then, without loss of generality, $\orient{\V{x}_i, \V{x}_j}_t=x_{i,t}$ and $\orient{\V{y}_i, \V{y}_j}_t=y_{i,t}$ for a threshold $\epsilon$, typically $\epsilon > 0.5$.

	\item If $\card{\Set{t^{\prime}}{l_{i,t^{\prime}} > l_{j,t^{\prime}}, \, t^{\prime} \in [t-w \rangedots t+w]}} > w$, then, without loss of generality, $\orient{\V{x}_i, \V{x}_j}_t=x_{i,t}$ and $\orient{\V{y}_i, \V{y}_j}_t=y_{i,t}$, for a window of size $2w+1$.

	\item If $l_{i,t} > l_{j,t}$ and if the nearest confident left orientation is closer than the nearest confident right orientation, i.e., $$\argmin_{t^{\prime}} \Set{\abs{t-t^{\prime}}}{l_{i,t^{\prime}} - l_{j,t^{\prime}} \geq \epsilon} > \argmin_{t^{\prime}} \Set{\abs{t - t^{\prime}}}{l_{j,t^{\prime}} - l_{i,t^{\prime}} \geq \epsilon},$$ then, without loss of generality, $\orient{\V{x}_i, \V{x}_j}_t=x_{i,t}$ and $\orient{\V{y}_i, \V{y}_j}_t=y_{i,t}$.
	\item If simply $l_{i,t} > l_{j,t}$, then without loss of generality, $\orient{\V{x}_i, \V{x}_j}_t=x_{i,t}$ and $\orient{\V{y}_i, \V{y}_j}_t=y_{i,t}$.
\end{itemize}

Except directly comparing the prediction confidence scores as in the last procedure, some of the time points might be left with undecided orientations.
If the number of such time points is acceptable, then directly comparing the prediction scores for those time points is convenient and handy.

After applying the above procedures for a left and right counterpart pair $i$ and $j$, we can define oriented multivariate time series as;
\begin{align*}
	\V{X}^{\omicron} & = \rbr{\sqbr{\rbr{\V{x}_k}_{k \notin \bigcup_{\set{i,j} \in \mathcal{O}} \set{i,j}}}^\top \concat \sqbr{\rbr{\orient{\V{x}_i, \V{x}_j}}_{\set{i,j} \in \mathcal{O}}}^\top}, \\
	\V{Y}^{\omicron} & = \rbr{\sqbr{\rbr{\V{y}_k}_{k \notin \bigcup_{\set{i,j} \in \mathcal{O}} \set{i,j}}}^\top \concat \sqbr{\rbr{\orient{\V{y}_i, \V{y}_j}}_{\set{i,j} \in \mathcal{O}}}^\top},
\end{align*}
where $\mathcal{O}$ is the set of index pairs of left and right counterparts and $\bigcup_{\set{i,j} \in \mathcal{O}} \set{i,j}$ is the union of all indexes of such body parts pairs.
Applying described procedures for each left and right counterparts results in computing $\V{X}^{\omicron}$ and $\V{Y}^{\omicron}$.
This oriented versions of original data matrices can be used instead of $\V{X}$ and $\V{Y}$ in the rest of the pipeline, if desired.

\subsubsection{Detecting Occlusions Using Prediction Scores and Preternatural Tracking Predictions}

\subsection{Aligning Different Orientations}

\subsection{Filtering and Imputation}

\section{Computation of Spatio-temporal Features}
After preprocessing of pose values, it is now feasible to go one step further towards learning stereotypical behaviors.
Although tracking of relevant body parts and processing corresponding pose values is an essential step for quantifying behavior, a set of coordinate values is not sufficient to represent and capture complex spatio-temporal dynamics of animal behavior.
There are thousands of unique postures, and behaviors are not even exhibited by some static set of postures.
Instead, they are defined by expressive and meaningful spatio-temporal features such as distances, velocities, angles and angular velocities.
Therefor, one need to compute such features from the coordinate values of body parts in two-dimensional space.

The second stage of the feature extraction is computation of spatio-temporal features from pose values. Two type of features are computed in this stage, as listed below.
\begin{enumerate}
	\item \textbf{Snapshot features:} Spatio-temporal feature values computed at a snapshot of time, listed as follows:
	      \begin{itemize}
		      \item distances,
		      \item angles,
		      \item cartesian pose values (i.e., per body part features).
	      \end{itemize}
	\item \textbf{Gradient features:} Spatio-temporal feature values computed based how snapshot features change over time, listed as follows:
	      \begin{itemize}
		      \item change of distances,
		      \item change of angles (i.e., angular velocities),
		      \item change of cartesian pose values (i.e., body part velocities).
	      \end{itemize}
\end{enumerate}

The gradients of snapshot features are computed using second-order accurate central differences in the interior points.
The resulting gradient features have the same shape, i.e., the number of features and the number of time-stamps, as the snapshot features.

\subsection{Distances Between Body Parts}
Given a body part pair $\rbr{i,j}$, the distance between them at a time step $t$ is calculated as a usual Euclidean distance, given below.
\begin{equation}
	d^{i,j}_t = \sqrt{(x_{i,t} - x_{j,t})^2 + (y_{i,t} - y_{j,t})^2}.
\end{equation}

The corresponding gradient feature, namely the change of distance between body part $i$ and $j$, is computed using the second order gradient approximation,
\begin{equation}
	\dot{d}^{i,j}_t = \begin{cases} \frac{\abs{d^{i,j}_{t+1} - d^{i,j}_{t}}}{\Delta t} & \textnormal{if } $t=0$ \textnormal{ or } $t=T$, \\ \frac{\abs{d^{i,j}_{t+1} - d^{i,j}_{t-1}}}{2\Delta t} & \textnormal{otherwise}, \end{cases}
\end{equation}
where $\Delta t$ is the sampling period, and it is equal to $\sfrac{1}{\textnormal{FPS}}$ seconds.

\subsection{Joint Angles Between Body Parts}
Given a triplet of body parts $\rbr{i,j,k}$, angle between $i$ and $k$ around $j$ is computed using the 2-argument $\operatorname{arctangent}$ function as given below.
\begin{equation}
	w^{i,j,k}_t = \mathop{\rm{atan2}} \rbr{\det \begin{bmatrix} x_{i,t} - x_{j,t}, x_{k,t} - x_{j,t} \\ y_{i,t} - y_{j,t}, y_{k,t} - y_{j,t} \end{bmatrix}, \begin{bmatrix} x_{i,t} - x_{j,t} \\ y_{i,t} - y_{j,t} \end{bmatrix} \cdot \begin{bmatrix} x_{k,t} - x_{j,t}  \\ y_{k,t} - y_{j,t} \end{bmatrix}} + \pi.
\end{equation}

Then, similar to the change of distance features, angular velocities are approximated by,
\begin{equation}
	\dot{w}^{i,j,k}_t = \begin{cases} \frac{\abs{w^{i,j,k}_{t+1} - w^{i,j,k}_{t}}}{\Delta t} & \textnormal{if } $t=0$ \textnormal{ or } $t=T$, \\ \frac{\abs{w^{i,j,k}_{t+1} - w^{i,j,k}_{t-1}}}{2\Delta t} & \textnormal{otherwise}. \end{cases}
\end{equation}

\subsection{Cartesian Pose Values of Body Parts}
Cartesian pose values of a body part $i$ is straightforwardly given by the $x$ and $y$ coordinate values as follows,
\begin{align}
	x^{i}_t & = x_{i,t}, \\
	y^{i}_t & = y_{i,t}.
\end{align}

Note that for a single body part, two feature values are generated.
Corresponding gradient features, namely the body part velocities along each cartesian component, are computed by
\begin{align}
	\dot{x}^{i}_t & = \begin{cases} \frac{x^{i}_{t+1} - x^{i}_{t}}{\Delta t} & \textnormal{if } $t=0$ \textnormal{ or } $t=T$, \\ \frac{x^{i}_{t+1} - x^{i}_{t-1}}{2\Delta t} & \textnormal{otherwise}, \end{cases} \\
	\dot{y}^{i}_t & = \begin{cases} \frac{y^{i}_{t+1} - y^{i}_{t}}{\Delta t} & \textnormal{if } $t=0$ \textnormal{ or } $t=T$, \\ \frac{y^{i}_{t+1} - y^{i}_{t-1}}{2\Delta t} & \textnormal{otherwise}. \end{cases}
\end{align}

In order to compute overall two-dimensional velocity of a body part, one can always use the distance between origin and corresponding body part.

\subsection{Constructing Spatio-temporal Feature Matrices}
Let $\mathcal{C}, \mathcal{D}$, and $\mathcal{A}$ denote the sets of body parts, body part pairs, and body part triplets; respectively defining cartesian pose values, distances, and angles.
Similarly, let $\mathcal{C}^{\prime}, \mathcal{D}^{\prime}$, and $\mathcal{A}^{\prime}$ denote sets which define sets of respective gradient features.
Then snapshot feature matrix $S$ constructed as follows;
\begin{equation}
	\V{S} = \rbr{\rbr{x^i_t}_{1 \leq t \leq T, \, i \in \mathcal{C}} \concat \rbr{y^i_t}_{1 \leq t \leq T, \, i \in \mathcal{C}} \concat \rbr{d^{i,j}_t}_{1 \leq t \leq T, \, \cbr{i,j} \in \mathcal{D}} \concat \rbr{w_t^{i,j,k}}_{1 \leq t \leq T, \, \cbr{i,j,k} \in \mathcal{A}}} .
\end{equation}

Similarly, for gradient features, the feature matrix is constructed by concatenating change of distances, angular velocities and body part velocities; given by
\begin{equation}
	\V{G} = \rbr{\rbr{\dot{x}^i_t}_{1 \leq t \leq T, \, i \in \mathcal{C}^{\prime}} \concat \rbr{\dot{y}^i_t}_{1 \leq t \leq T, \, i \in \mathcal{C}^{\prime}} \concat \rbr{d^{i,j}_t}_{1 \leq t \leq T, \, \cbr{i,j} \in \mathcal{D}^{\prime}} \concat \rbr{\dot{w}_t^{i,j,k}}_{1 \leq t \leq T, \, \cbr{i,j,k} \in \mathcal{A}^{\prime}} }.
\end{equation}

The resulting two feature matrices are $\V{S} \in \mathbb{R}^{T \times \rbr{2\card{\mathcal{C}}+\card{\mathcal{D}}+\card{\mathcal{A}}}}$ and $\V{G} \in \mathbb{R}^{T \times \rbr{2\card{\mathcal{C}^{\prime}}+\card{\mathcal{D}^{\prime}}+\card{\mathcal{A}^{\prime}}}}$.
Let $N_S$ denote the number of snapshot features, being equal to $2\card{\mathcal{C}}+\card{\mathcal{D}}+\card{\mathcal{A}}$ and let $N_G$ denote the number of gradient features, which is equal to $2\card{\mathcal{C}^{\prime}}+\card{\mathcal{D}^{\prime}}+\card{\mathcal{A}^{\prime}}$.

\section{Computation of Dynamic Postural Features}\label{section:dynamic-postural-features}
Instantaneous values of spatio-temporal features do not provide a sufficient description of complex postural dynamics of behaviors.
Understanding the output of a complex biological system, in our case behavior, can only be achieved by studying multiple time-scales together.
Previous studies attempt to search behavioral motifs, e.g., repeated sub-sequences of actions with finite length, within the behavioral time series \CITE.
However, as \citet{berman_mapping_2014} states, this paradigm usually requires problems of temporal alignment and relative phasing between different scales.
Alternatively, extending spatio-temporal features to capture postural dynamics at different time-scales eliminate requirements of temporal alignment and motif based analysis.
In order to extend the spatio-temporal features to dynamic postural features, we applied wavelet transformation (similar to \citet{berman_mapping_2014}) and computed moving statistics at different time-scales (similar to \citet{kabra_jaaba_2013}), respectively for snapshot feature set ($\V{S}$) and gradient feature set ($\V{G}$).

\subsection{Moving Statistics of Gradient Features}
Gradient features only reflect the instantaneous values of velocities with respect to the sampling rate.
In order to capture how these values change within a given interval, the moving statistics, e.g., mean and standard deviation, of gradient features are computed with a sliding window approach.
Let $\tau$ be the window size parameter, i.e., the timescale of interest, then the moving mean of the corresponding gradient feature $\V{g}_{i}$ is given by the function $\mu_\tau$;
\begin{equation}
	\mu_{\tau}(g_{i,t}) = \sfrac{1}{\min\{t+\tau,T\}-\max\{t-\tau,1\}+1} \sum_{t^{\prime}=\max\{t-\tau,1\}}^{\min\{t+\tau,T\}} g_{i, t^{\prime}}.
\end{equation}

Similarly, the moving standard deviation of a gradient feature $g_{i,t}$ by is computed by $\sigma_\tau$, as in the below equation.
\begin{equation}
	\sigma_\tau(g_{i,t}) = \rbr{\sfrac{1}{\min\{t+\tau,T\}-\max\{t-\tau,1\}+1} \sum_{t^{\prime}=\max\{t-\tau,1\}}^{\min\{t+\tau,T\}} \rbr{\mu_{\tau}(g_{i,t}) - g_{i, t^{\prime}}}^2}^{\sfrac{1}{2}}
\end{equation}

Moving statistics feature generation approach is used to learn animal behavior by \citet{kabra_jaaba_2013}, and \citet{marshall_continuous_2021} is also included such features into the analysis.

\subsection{Wavelet Transformation of Snapshot Features}
The wavelet domain is a useful representation of postural dynamics due to the following reasons given by \citet{berman_mapping_2014}, and proposed spectrogram generation is used by others as well \citep{marshall_continuous_2021, todd_systematic_2017}.
\begin{itemize}
	\item It describes dynamics over multiple time-scales simultaneously by possessing a multi-resolution time-frequency trade-off.
	\item It eliminates the requirement of precise temporal alignment for capturing periodic behaviors by taking amplitudes of the continuous wavelet transform of each snapshot feature at different scales.
\end{itemize}
Given a function $s(t)$, continuous wavelet transformation at a frequency $f>0$ is expressed by the following integral;
\begin{equation}
	W_{f,t^{\prime}}\sqbr{s\rbr{t}} = \frac{1}{\sqrt{a\rbr{f}}} \int_{-\infty}^{\infty} s\rbr{t} \mathit{\Psi}^{\ast}\rbr{\frac{t - t^{\prime}}{a(f)}} \dd{t},
\end{equation}
where $\Psi$ is the wavelet function and $a$ is a function for converting frequencies to wavelet scale factor.
The Morlet wavelet is suitable for describing postural dynamics which is closely related to human perception, both hearing and vision \citep{daugman_uncertainty_1985}, and is used in the pipeline.
The corresponding wavelet function is given by
\begin{equation}
	\Psi(t) = \exp{\frac{t^2}{2}} \cos(w_0t),
\end{equation}
where $w_0$ is a non-dimensional parameter. The frequency to scale conversion function $a$ for Morlet wavelet is as follows;
\begin{equation}
	a\rbr{f} = \frac{w_0 + \sqrt{2+w_0^2}}{4 \pi f},.
\end{equation}

For the discrete sequence of snapshot feature $\V{s}_i$ with sampling period $\Delta t$, $W_{f,t^\prime}[s(t)]$ translates into;
\begin{equation}
	\label{eq:wavelet-practical}
	W_f\rbr{\V{s}_i, t^\prime} = \frac{1}{\sqrt{a(f)}} \sum_{t=1}^{T} {\Delta t} s_{i,t} \mathit{\Psi}^{\ast}\rbr{\frac{t-t^{\prime}}{a{f}}},
\end{equation}
where $t^\prime, t \in \mathbb{Z}$ and $ 1 \leq t^\prime \leq T$ \citep{torrence_practical_1998}.

\subsubsection{Normalization of Wavelet Power Spectrum}
In order to ensure that wavelet transforms (Equation~\ref{eq:wavelet-practical}) at each frequency $f$ are directly comparable to each other and to the other transformed time series, the transformation $W_f$ has to be normalized at each frequency $f$ to have unit energy.
This normalization for Morlet wavelet at frequency $f$ is given by
\begin{equation}
	C(f) = \frac{\pi^{-\frac{1}{4}}}{\sqrt{2a(f)}}\exp{\sfrac{1}{4}\rbr{w_0-\sqrt{w_0^2+2}}^2}.
\end{equation}
So resulting normalized transformation which is also used to generate the spectrogram in \citet{berman_mapping_2014} is given by
\begin{equation}
	W^0_f\rbr{\V{s}_i, t^\prime} = \frac{1}{C\rbr{f}} \abs{W_f\rbr{\V{s}_i, t^\prime}}.
\end{equation}

In addition to the above conventionally used normalization, we alternatively adopted the normalization proposed by \citet{liu_rectification_2007}. According to this alternative adjustment, the wavelet power spectrum should be equal to the transform coefficient squared divided by the scale it associates.
\begin{equation}
	W^{0}_f\rbr{\V{s}_i, t^\prime} = \frac{W_f\rbr{\V{s}_i, t^\prime}^2}{a\rbr{f}}
\end{equation}
We observed substantial improvements using this power spectrum (see Section~\ref{section:employing-proposed-pipeline}).

\subsubsection{Determining Spectrum Frequencies}
We investigate two different approaches for computing a set of frequencies and, we include both of them in the behavior mapping pipeline.
One set is dyadically spaced frequencies between $f_{\textnormal{min}}$ and $f_{\textnormal{max}}$ via;
\begin{equation}
	f_i = f_{\textnormal{max}} 2^{-\frac{i-1}{N_f-1}\log{\frac{f_{\textnormal{max}}}{f_{\textnormal{min}}}}},
\end{equation}
where $f_{\textnormal{max}} = \sfrac{FPS}{2} \textnormal{ Hz}$ is the Nyquist frequency.

The other alternative set of frequencies is linearly spaced between $f_{\textnormal{min}}$ and $f_{\textnormal{max}}$ by
\begin{equation}
	f_i = f_{\textnormal{min}} + \frac{f_{\textnormal{max}} - f_{\textnormal{min}}}{N_f-1}i,
\end{equation}
for $i=1,2,\dots,N_f$, and their corresponding wavelet scales are computed via $a\rbr{f_i}$.

\subsection{Constructing Dynamic Postural Feature Tensors}
Let $\mathcal{T}_S=\set{\sfrac{1}{f_{\textnormal{min}}}, \cdots \sfrac{1}{f_{\textnormal{max}}}}$ and $\mathcal{T}_G=\set{\tau_{\textnormal{min}}, \cdots, \tau_{\textnormal{max}}}$ denote the time-scale sets respectively for wavelet transforms of snapshot features and moving statistics of gradient features. Then corresponding feature tensors are given as follows:
\begin{align}
	\V{W}        & = \sqbr{W^0_{\sfrac{1}{f}}\rbr{\V{s}_i, t}}_{1 \leq i \leq N_S, \,1 \leq t \leq T, \, \sfrac{1}{f} \in \mathcal{T}_S}, \\
	\V{M}_\mu    & = \sqbr{\mu_\tau\rbr{g_{i,t}}}_{1 \leq i \leq  N_G,  \, 1\leq t \leq T, \, \tau \in \mathcal{T}_G},                    \\
	\V{M}_\sigma & = \sqbr{\sigma_\tau\rbr{g_{i,t}}}_{1 \leq i \leq N_G, \, 1\leq t \leq T, \, \tau \in \mathcal{T}_G},
\end{align}
where $\V{S} \in \mathbb{R}^{T \times N_S}$ and $\V{G} \in \mathbb{R}^{T \times N_G}$.

The resulting extended feature tensors of dynamic postural representations are $\V{W} \in \mathbb{R}^{T \times \card{\mathcal{T}_s} \times N_S}$, $\V{M}_\mu \in \mathbb{R}^{T \times \card{\mathcal{T}_v} \times N_G}$ and $\V{M}_\sigma \in \mathbb{R}^{T \times \card{\mathcal{T}_v} \times N_G}$.

\section{Computation of Behavioral Representations}
After applying wavelet transformation or computing moving statistics to extend extracted spatio-temporal features to dynamic postural features, a couple of additional operations are required to continue in the behavior mapping pipeline.

\subsection{Flattening Dynamic Postural Feature Tensors}
As constructed in Section~\ref{section:dynamic-postural-features}, dynamic postural feature tensors are $\V{W} \in \mathbb{R}^{T \times \card{\mathcal{T}_S} \times N_S}$, $\V{M}_\mu \in \mathbb{R}^{T \times \card{\mathcal{T}_G} \times N_G}$, and  $\V{M}_\sigma \in \mathbb{R}^{T \times \card{\mathcal{T}_G} \times N_G}$.
In order to apply manifold learning-based dimensionality reduction algorithms or traditional machine learning algorithms such as decision trees, the last two dimensions of these feature tensors needs to be flattened.
As a result, feature matrices are $\V{W}^\ast \in \mathbb{R}^{T \times \rbr{ N_S\card{\mathcal{T}_S}}}, \V{M}_\mu^\ast \in \mathbb{R}^{T \times \rbr{ N_G\card{\mathcal{T}_G}}}$ and $\V{M}_\sigma^\ast \in \mathbb{R}^{T \times \rbr{ N_G\card{\mathcal{T}_G}}}$ are obtained.

\subsection{\texorpdfstring{$\textnormal{L}_1$}{L1} Normalization of Frames}
Dynamic postural feature distributions of similar behaviors may differ among flies due to different characteristics such as gender, and being sleep-deprived.
Due to the two-dimensional nature of the video recordings, different orientations may cause observing different feature values for the same behavior.
In order to have a homogeneous feature space among flies and along the time, at each time step $t$, $\textnormal{L}_1$ normalization is applied as follows;
\begin{align}
	\V{\hat{w}}_i & = \rbr{\frac{w^\ast_{t,i}}{\sum_{j=1}^{N_S \card{\mathcal{T}_s}}w^\ast_{t, j}}}_{1 \leq t \leq T}, \\
	\V{\hat{W}}   & = \sqbr{\rbr{\V{w}_i}_{1 \leq i \leq N_S\card{\mathcal{T}_S}}}.
\end{align}
Similarly, $\textnormal{L}_1$ normalized versions of $\V{M}^\ast_\mu$ and $\V{M}^\ast_\sigma$, namely $\V{\hat{M}}_\mu$ and $\V{\hat{M}}_\sigma$ are obtained.
Here $\V{\hat{W}}$, $\V{\hat{M}}_\mu$ and $\V{\hat{M}}_\sigma$ are the final multivariate time series of normalized high dimensional behavioral representation of a single experiment data, i.e., single fruit fly recorded between ZT10 and ZT2. Notice that, we may treat each time step, i.e., frame, as a discrete probability distribution after $\textnormal{L}_1$ normalization.

\section{Summary}
